# パラメータ分析ガイド

本ドキュメントは `fusi` の全国生産に向けたパラメータ最適化を目的として、各パラメータの役割・評価指標・推奨設定・リスクを体系的に整理したものです。

## 目次

1. [分析対象パラメータ一覧](#分析対象パラメータ一覧)
2. [コマンド／アプリケーション引数](#コマンドアプリケーション引数)
3. [出力形式／オプション](#出力形式オプション)
4. [OS / ストレージ / ファイルシステム設定](#os--ストレージ--ファイルシステム設定)
5. [リソース制約・並列性](#リソース制約並列性)
6. [監視・ロギング](#監視ロギング)
7. [実験／評価フロー](#実験評価フロー)
8. [推奨プロファイル設定](#推奨プロファイル設定)
9. [適用例](#適用例)

---

## 分析対象パラメータ一覧

| グループ | パラメータ | 影響領域 | 優先度 |
|---------|-----------|---------|--------|
| コマンド引数 | TMPDIR | I/O レイテンシ | 高 |
| コマンド引数 | GDAL_CACHEMAX | メモリ・読み取り I/O | 高 |
| コマンド引数 | --warp-threads | CPU / I/O バランス | 高 |
| コマンド引数 | --min-zoom / --max-zoom | タイル数・処理時間 | 中 |
| コマンド引数 | --bbox | I/O 負荷・処理範囲 | 中 |
| コマンド引数 | --io-sleep-ms | I/O バースト緩和 | 中 |
| コマンド引数 | --fsync-interval-tiles | データ整合性 | 中 |
| コマンド引数 | --overwrite | 再実行方針 | 低 |
| 出力形式 | 入力 DEM 選択順序 | I/O 量・マージ戦略 | 中 |
| OS 設定 | vm.dirty_ratio 等 | writeback 動作 | 低 |
| OS 設定 | I/O スケジューラ | I/O 優先度 | 低 |
| OS 設定 | ファイルシステム | 書き込みパターン | 低 |

---

## コマンド／アプリケーション引数

### TMPDIR（作業一時ディレクトリの場所）

| 項目 | 内容 |
|------|------|
| **評価対象** | 一時 I/O のレイテンシと IOPS、fsync の影響 |
| **指標** | ディスク書き込みレイテンシ（ms）、書込スループット（MB/s）、プロセスの D 状態の頻度 |
| **現在の既定値** | `$PWD/output`（justfile で設定） |
| **試験案** | 1. TMPDIR=作業 SSD<br>2. TMPDIR=同一 SSD<br>3. TMPDIR=tmpfs |
| **期待される効果** | 高速デバイスに移すほど fsync の影響は減る。tmpfs は最大速度だがクラッシュ時のデータ喪失増加 |
| **推奨レンジ** | 出力先と同じ外付け SSD（容量に余裕がある場合） |
| **リスク** | tmpfs 使用時はクラッシュでデータ消失。システムボリュームに設定すると ENOSPC の危険 |

#### 設定例

```bash
# 外付け SSD に設定（推奨）
export TMPDIR="$PWD/output"

# tmpfs を使用（高速だがリスクあり）
export TMPDIR="/dev/shm"
```

---

### GDAL_CACHEMAX

| 項目 | 内容 |
|------|------|
| **評価対象** | GDAL がメモリ内でキャッシュするバイト数の影響 |
| **指標** | 総読み取り MB/s、ページキャッシュヒット率、メモリ使用量、処理時間 |
| **現在の既定値** | `512`（MB、justfile で設定） |
| **試験案** | 512 → 1024 → 2048 の比較。OOM の可能性チェック |
| **期待される効果** | 増やすと読み取り回数が減りスループット向上。ただしメモリ枯渇とスワップに注意 |
| **推奨レンジ** | 256〜1024 MB（マシンの RAM に応じて調整） |
| **リスク** | 大きすぎるとスワップ発生で逆効果。16GB RAM では 512〜1024 が安全圏 |

#### 設定例

```bash
# 控えめな設定（16GB RAM マシン）
export GDAL_CACHEMAX=512

# メモリに余裕がある場合
export GDAL_CACHEMAX=1024
```

---

### --warp-threads

| 項目 | 内容 |
|------|------|
| **評価対象** | 並列化度を上げた時の CPU 使用率、I/O 競合、スケーラビリティ |
| **指標** | CPU 使用率、context switch、I/O レイテンシ、合計処理時間 |
| **現在の既定値** | `1` |
| **試験案** | 1 → 2 → 4 → コア数/2 → コア数 で比較 |
| **期待される効果** | CPU バウンドなら有利。I/O がボトルネックなら増やすと I/O 競合で逆効果 |
| **推奨レンジ** | 1〜2（I/O ボトルネック環境では 1 を推奨） |
| **リスク** | スレッド数を増やしすぎると I/O 競合が増加し、全体のスループットが低下 |

#### 設定例

```bash
# I/O ボトルネック環境（外付け SSD など）
just aggregate dem1a --warp-threads 1

# CPU バウンド環境（NVMe など高速ストレージ）
just aggregate dem1a --warp-threads 4
```

---

### --min-zoom / --max-zoom

| 項目 | 内容 |
|------|------|
| **評価対象** | 生成するタイル数に応じた I/O/CPU 負荷および出力サイズ |
| **指標** | 生成タイル数、出力 PMTiles サイズ、処理時間 |
| **現在の既定値** | min-zoom: `0`、max-zoom: 自動（ソース解像度から計算） |
| **試験案** | min-zoom を上下して生成負荷変化を観察 |
| **期待される効果** | 低い zoom（タイル少）なら低負荷。高ズームはタイル爆発→I/O・fsync 頻度に影響 |
| **推奨レンジ** | min-zoom: 0〜6、max-zoom: ソース解像度に依存（dem1a: 16、dem10b: 13） |
| **リスク** | max-zoom を高くしすぎるとタイル数が指数的に増加し、処理時間・ストレージ消費が激増 |

#### ズームレベルとタイル数の関係

| ズーム範囲 | 概算タイル数（日本全域） | 処理時間目安 |
|-----------|------------------------|-------------|
| 0-12 | 〜10万 | 〜30分 |
| 0-14 | 〜100万 | 〜5時間 |
| 0-15 | 〜400万 | 〜20時間 |
| 0-16 | 〜1600万 | 〜3日 |

---

### --bbox

| 項目 | 内容 |
|------|------|
| **評価対象** | 領域サイズが I/O に与える影響 |
| **指標** | 処理対象タイル数、読み込みファイル数、処理時間 |
| **現在の既定値** | なし（ソース全域） |
| **試験案** | 県単位、地方単位での処理時間比較 |
| **期待される効果** | 小さな bbox で部分処理することでテスト・検証が容易に |
| **推奨レンジ** | テスト時は県単位（例：長崎県 `128.3,32.4,131.6,33.8`） |
| **リスク** | なし（処理範囲の制限のみ） |

#### 設定例

```bash
# 長崎県スモークテスト
just aggregate dem1a --bbox 128.3 32.4 131.6 33.8

# 九州地方
just aggregate dem1a --bbox 129.5 30.0 132.0 34.0
```

---

### --io-sleep-ms

| 項目 | 内容 |
|------|------|
| **評価対象** | I/O 間隔を短い休止で挟むことで I/O バーストを緩和できるか |
| **指標** | 瞬間的 I/O バースト、平均スループット、処理時間 |
| **現在の既定値** | `1`（ms） |
| **試験案** | 0（無）→ 1ms → 5ms → 10ms |
| **期待される効果** | 小さな sleep は割り込み/キューの高負荷を緩和し安定化するが総時間は伸びる |
| **推奨レンジ** | 0〜5 ms（安定性重視なら 1〜2 ms） |
| **リスク** | 大きな値を設定すると処理時間が線形に増加 |

#### 設定例

```bash
# 安定性重視（既定）
just aggregate dem1a --io-sleep-ms 1

# 速度優先（高速ストレージ向け）
just aggregate dem1a --io-sleep-ms 0
```

---

### --fsync-interval-tiles

| 項目 | 内容 |
|------|------|
| **評価対象** | fsync の頻度とデータ整合性・パフォーマンスのトレードオフ |
| **指標** | fsync 回数、処理時間、クラッシュ時の損失タイル数 |
| **現在の既定値** | `10000`（タイル） |
| **試験案** | 5000 → 10000 → 20000 → 0（無効） |
| **期待される効果** | 間隔を広げると I/O 効率向上。狭めるとクラッシュ時の損失減少 |
| **推奨レンジ** | 5000〜20000（データ重要度に応じて調整） |
| **リスク** | 0（無効）にするとクラッシュ時に大量のタイルを失う可能性 |

---

### --overwrite

| 項目 | 内容 |
|------|------|
| **評価対象** | 既存ファイルの上書き方針 |
| **指標** | 履歴管理、再実行安全性 |
| **現在の既定値** | `false`（上書き拒否） |
| **試験案** | なし |
| **期待される効果** | 誤って既存データを消さない安全策 |
| **推奨レンジ** | 通常は指定しない。意図的な再生成時のみ使用 |
| **リスク** | 指定すると既存の MBTiles が上書きされる |

---

## 出力形式／オプション

### 入力 DEM 選択（dem1a, dem5a, dem10b）

| 項目 | 内容 |
|------|------|
| **評価対象** | 高解像度 DEM の読み込み頻度、解像度差による I/O 量、マージ戦略 |
| **指標** | read MB/s、read ファイル数、平均読み取りサイズ |
| **試験案** | 順序を変えたり、不要な DEM 除外で比較 |
| **期待される効果** | 優先度順（dem1a → dem5a → dem10b）で高解像度データを優先 |
| **推奨順序** | `dem1a dem5a dem10b`（高解像度優先） |
| **リスク** | 順序を間違えると低解像度データで高解像度領域が埋まる |

#### DEM ソース特性

| ソース | 解像度 | 推奨 max-zoom | 用途 |
|--------|--------|--------------|------|
| dem1a | 約 1m | 16 | 最高精度が必要な領域 |
| dem5a | 約 5m | 14 | 中精度領域 |
| dem10b | 約 10m | 13 | 広域カバレッジ |

#### 設定例

```bash
# 優先度順で複数ソースを統合
just aggregate dem1a dem5a dem10b

# 高解像度のみ（処理時間短縮）
just aggregate dem1a
```

---

## OS / ストレージ / ファイルシステム設定

### ストレージ媒体

| 項目 | 内容 |
|------|------|
| **評価対象** | SSD のモデル・特性、接続方式（SATA/NVMe/USB） |
| **指標** | シーケンシャル/ランダム read/write 速度、IOPS |
| **期待される効果** | NVMe > SATA SSD > USB 3.0 SSD の順で高速 |
| **推奨** | 可能であれば NVMe SSD、次点で SATA SSD |
| **リスク** | USB 接続は帯域制限あり。長時間書き込みで発熱による速度低下も |

#### 現在の環境

- Mac mini 16GB RAM
- USB 3.0 SSD 3TB

---

### ファイルシステム

| 項目 | 内容 |
|------|------|
| **評価対象** | ext4/xfs/btrfs/APFS とマウントオプション |
| **指標** | 書き込みスループット、fsync レイテンシ |
| **試験案** | noatime、data=writeback（ext4）などのオプション比較 |
| **期待される効果** | noatime でメタデータ更新を削減 |
| **推奨** | ext4（Linux）または APFS（macOS）、noatime オプション |
| **リスク** | data=writeback はクラッシュ時のデータ破損リスク増 |

---

### カーネルの書き込みバッファ設定

| パラメータ | 説明 | 既定値 | 推奨調整 |
|-----------|------|--------|---------|
| vm.dirty_ratio | 全メモリに対する dirty ページの最大比率 | 20% | 30〜40%（メモリに余裕があれば） |
| vm.dirty_background_ratio | バックグラウンド writeback 開始閾値 | 10% | 15〜20% |
| dirty_expire_centisecs | dirty ページの有効期限 | 3000 | 調整不要 |

#### 設定例（Linux）

```bash
# 一時的な変更
sudo sysctl -w vm.dirty_ratio=30
sudo sysctl -w vm.dirty_background_ratio=15

# 永続化（/etc/sysctl.conf に追記）
vm.dirty_ratio=30
vm.dirty_background_ratio=15
```

---

### tmpfs の使用

| 項目 | 内容 |
|------|------|
| **評価対象** | RAM ディスクでの一時ファイル処理 |
| **指標** | 書き込みレイテンシ、スループット |
| **期待される効果** | 最大の速度（ディスク I/O なし） |
| **推奨** | 十分な RAM があり、データ損失を許容できる場合のみ |
| **リスク** | クラッシュ時にデータ全損。RAM を圧迫しスワップを誘発する可能性 |

---

## リソース制約・並列性

### CPU コア数 / NUMA トポロジー

| 項目 | 内容 |
|------|------|
| **評価対象** | 利用可能な CPU コアと --warp-threads の関係 |
| **指標** | CPU 使用率、スレッド間の競合 |
| **期待される効果** | コア数に応じた適切なスレッド数設定で効率向上 |
| **推奨** | I/O ボトルネック環境では warp-threads を 1〜2 に抑制 |

---

### メモリ量（RAM）と swap

| 項目 | 内容 |
|------|------|
| **評価対象** | 利用可能メモリと GDAL_CACHEMAX の関係 |
| **指標** | メモリ使用量、スワップ発生頻度 |
| **期待される効果** | スワップを避けることで安定した処理速度を維持 |
| **推奨** | GDAL_CACHEMAX + OS/アプリ使用量 < 利用可能 RAM |

#### メモリ配分ガイドライン（16GB RAM の場合）

| 用途 | 推奨配分 |
|------|---------|
| OS + システム | 2〜4 GB |
| Python プロセス | 2〜4 GB |
| GDAL_CACHEMAX | 512 MB〜1 GB |
| ページキャッシュ | 残り（4〜8 GB） |

---

### ulimit（open files）

| 項目 | 内容 |
|------|------|
| **評価対象** | 同時オープンファイル数の制限 |
| **指標** | "Too many open files" エラーの発生 |
| **期待される効果** | 大量のソースファイル処理時のエラー回避 |
| **推奨** | `ulimit -n 4096` 以上 |

#### 設定例

```bash
# 現在の制限を確認
ulimit -n

# 一時的に拡大
ulimit -n 4096
```

---

## 監視・ロギング

### モニタリング指標の取得

| ツール | 取得指標 | 使用例 |
|--------|---------|--------|
| iostat | I/O スループット、レイテンシ | `iostat -x 1` |
| iotop | プロセス別 I/O 使用量 | `iotop -aoP` |
| vmstat | メモリ、スワップ、CPU | `vmstat 1` |
| dstat | 総合リソース監視 | `dstat -cdnm` |
| pidstat | プロセス別 CPU/メモリ | `pidstat -d 1` |

### 推奨モニタリングコマンド

```bash
# 1秒間隔でディスク I/O を監視
iostat -x 1

# プロセス別 I/O を監視
sudo iotop -aoP

# 総合監視（別ターミナルで実行）
dstat -cdnm --top-io --top-bio
```

### プロセス D 状態の確認

```bash
# D 状態（ディスク I/O 待ち）のプロセス数を確認
ps aux | awk '$8 ~ /D/ {print}' | wc -l
```

---

## 実験／評価フロー

### ベースライン取得手順

1. **現在の設定でモニタリング開始**
   ```bash
   # 別ターミナルでモニタリング
   iostat -x 1 > iostat.log &
   vmstat 1 > vmstat.log &
   ```

2. **小規模テスト実行**
   ```bash
   # 長崎県でスモークテスト
   just aggregate dem1a --bbox 128.3 32.4 131.6 33.8 --max-zoom 14
   ```

3. **取得項目**
   - read/write MB/s
   - IOPS
   - 平均レイテンシ
   - プロセス D 状態比率
   - CPU メトリクス
   - RAM 使用量

### 1パラメータずつ変更

1. パラメータを1つだけ変更
2. 同じテストデータで再実行
3. 変更前後の差分を比較
4. 主要指標を元にトレードオフ決定

### 複数パラメータの同時調整例

```bash
# GDAL_CACHEMAX を増やして warp-threads を減らす
GDAL_CACHEMAX=1024 just aggregate dem1a --warp-threads 1
```

---

## 推奨プロファイル設定

### 安定性優先プロファイル

長時間の安定動作を優先する設定です。外付け USB SSD での運用に適しています。

```bash
export TMPDIR="$PWD/output"
export GDAL_CACHEMAX=512

just aggregate dem1a dem5a dem10b \
  --max-zoom 16 \
  --warp-threads 1 \
  --io-sleep-ms 1 \
  --fsync-interval-tiles 10000 \
  --progress-interval 100
```

### 速度優先プロファイル

高速ストレージ（NVMe など）での処理時間短縮を優先する設定です。

```bash
export TMPDIR="$PWD/output"
export GDAL_CACHEMAX=1024

just aggregate dem1a dem5a dem10b \
  --max-zoom 16 \
  --warp-threads 4 \
  --io-sleep-ms 0 \
  --fsync-interval-tiles 20000 \
  --progress-interval 500
```

### 耐久性優先プロファイル

クラッシュ時のデータ損失を最小限に抑える設定です。

```bash
export TMPDIR="$PWD/output"
export GDAL_CACHEMAX=512

just aggregate dem1a dem5a dem10b \
  --max-zoom 16 \
  --warp-threads 1 \
  --io-sleep-ms 2 \
  --fsync-interval-tiles 5000 \
  --progress-interval 100
```

---

## 適用例

### M4 Mac mini (16GB RAM) + USB 3.0 SSD 環境

**環境特性**
- CPU: M4 チップ（高性能、8〜10コア）
- RAM: 16GB（メモリ溢れに注意が必要）
- ストレージ: USB 3.0 SSD 3TB（I/O がボトルネックになる可能性が高い）
- 目標: 多少の速度向上を求めつつ、実行失敗を回避

**パラメータ選定の推論**

1. **GDAL_CACHEMAX=512**
   - 16GB RAM 環境では 1024 は危険。OS（2〜3GB）+ Python プロセス（2〜3GB）+ ページキャッシュ（4〜6GB）を考慮すると、512MB が安全圏
   - メモリ溢れのリスクを最小化しつつ、読み取りキャッシュの恩恵を受ける

2. **--warp-threads 2**
   - M4 の CPU パワーを活かしつつ、USB 3.0 SSD の I/O 競合を抑制
   - warp-threads=1 では CPU が遊ぶが、4 では USB 3.0 の帯域（実効 200〜400 MB/s）を複数スレッドで奪い合い逆効果
   - 2 スレッドで適度な並列性を確保しながら I/O キューの輻輳を回避

3. **--io-sleep-ms 1**
   - 完全に無効（0）にすると USB 3.0 コントローラが I/O バーストで飽和する危険
   - 1ms の微小スリープで I/O を整流し、安定性を確保
   - 処理時間への影響は軽微（全体の 0.1〜0.3% 程度）

4. **--fsync-interval-tiles 10000**
   - 20000 は高速ストレージ向けの設定。USB 3.0 では fsync の影響が大きいため、中間値が安全
   - クラッシュ時の損失を約 100〜200MB 程度に抑える（タイル平均 10〜20KB 想定）

5. **--progress-interval 1000**
   - 進捗表示の頻度を適度に抑え、ログ出力による I/O への影響を軽減

**推奨設定**

```bash
export TMPDIR="$PWD/output"
export GDAL_CACHEMAX=512

just aggregate dem1a dem5a dem10b \
  --warp-threads 2 \
  --io-sleep-ms 1 \
  --fsync-interval-tiles 10000 \
  --progress-interval 1000
```

**期待される効果**
- 既定設定（warp-threads=1）から **10〜30% の処理時間短縮**を期待
- メモリ使用量は 8〜10GB 程度で安定（スワップ発生なし）
- USB 3.0 SSD の I/O スループットを 60〜80% 程度で安定稼働
- 実行失敗のリスクを最小化

**監視すべき指標**

実行中は別ターミナルで以下を監視してください：

```bash
# メモリ使用量とスワップ（macOS）
vm_stat 1

# ディスク I/O（macOS）
iostat -w 1

# プロセスのメモリ使用量
top -pid $(pgrep -f aggregate_pmtiles) -stats pid,command,mem
```

**トラブルシューティング**

| 症状 | 原因 | 対処 |
|------|------|------|
| スワップ発生 | メモリ不足 | GDAL_CACHEMAX を 256 に削減 |
| I/O 待ち（D 状態）多発 | USB 3.0 飽和 | --io-sleep-ms を 2 に増加 |
| CPU 使用率 < 40% | スレッド不足 | --warp-threads を 3 に増加（様子見） |
| fsync で長時間停止 | USB 3.0 書き込み遅延 | --fsync-interval-tiles を 15000 に増加 |

---

## 参考情報

### 必要な追加情報

全国生産の最適化には以下の情報が必要です：

1. **実行マシンのスペック**
   - CPU コア数
   - RAM 容量
   - SSD モデルと接続方式（SATA/NVMe/USB）
   - ファイルシステム種類

2. **平均タイル／一時ファイルサイズ**
   - タイルあたりの平均バイト数
   - fsync 間隔の算出に必要

3. **許容できる最大クラッシュ時データ損失**
   - MB 単位での許容値
   - fsync 間隔の決定に影響

4. **望ましい優先度**
   - 性能最優先 / 安定性最優先 / 耐久性最優先

5. **利用可能な監視ツール**
   - iostat / iotop / vmstat の可用性

### 関連ドキュメント

- [README.md](./README.md) - プロジェクト概要
- [IMPLEMENTATION_SUMMARY.md](./IMPLEMENTATION_SUMMARY.md) - 実装詳細
- [pipelines/DIR_POLICY.md](./pipelines/DIR_POLICY.md) - ディレクトリ構成ポリシー
